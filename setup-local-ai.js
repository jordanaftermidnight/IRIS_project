#!/usr/bin/env node

/**
 * IRIS Local AI Setup Script
 * Automatically downloads and configures Mistral 7B as the default local AI model
 * 
 * @author Jordan After Midnight
 * @copyright 2025 Jordan After Midnight. All rights reserved.
 */

import { spawn } from 'child_process';
import { promisify } from 'util';
import fs from 'fs';

const exec = promisify(require('child_process').exec);

class LocalAISetup {
    constructor() {
        this.defaultModel = 'mistral:7b';
        this.backupModels = ['llama3.1:8b', 'llama3.2:latest'];
        this.ollamaHost = 'http://localhost:11434';
    }

    async checkOllamaInstalled() {
        try {
            await exec('ollama --version');
            console.log('‚úÖ Ollama is installed');
            return true;
        } catch (error) {
            console.log('‚ùå Ollama is not installed');
            console.log('üìã Please install Ollama first:');
            console.log('   macOS: brew install ollama');
            console.log('   Linux: curl -fsSL https://ollama.com/install.sh | sh');
            console.log('   Windows: Download from https://ollama.com/download');
            return false;
        }
    }

    async checkOllamaRunning() {
        try {
            const response = await fetch(`${this.ollamaHost}/api/version`);
            if (response.ok) {
                console.log('‚úÖ Ollama service is running');
                return true;
            }
        } catch (error) {
            console.log('‚ùå Ollama service is not running');
            console.log('üöÄ Starting Ollama service...');
            
            // Try to start Ollama service
            try {
                const ollamaProcess = spawn('ollama', ['serve'], {
                    detached: true,
                    stdio: 'ignore'
                });
                ollamaProcess.unref();
                
                // Wait a bit for service to start
                await new Promise(resolve => setTimeout(resolve, 3000));
                
                const retryResponse = await fetch(`${this.ollamaHost}/api/version`);
                if (retryResponse.ok) {
                    console.log('‚úÖ Ollama service started successfully');
                    return true;
                }
            } catch (startError) {
                console.log('‚ö†Ô∏è  Could not auto-start Ollama. Please run: ollama serve');
                return false;
            }
        }
        return false;
    }

    async getInstalledModels() {
        try {
            const response = await fetch(`${this.ollamaHost}/api/tags`);
            const data = await response.json();
            return data.models?.map(m => m.name) || [];
        } catch (error) {
            console.log('‚ö†Ô∏è  Could not fetch installed models');
            return [];
        }
    }

    async downloadModel(modelName) {
        console.log(`üì• Downloading ${modelName}...`);
        
        return new Promise((resolve, reject) => {
            const downloadProcess = spawn('ollama', ['pull', modelName], {
                stdio: ['inherit', 'pipe', 'pipe']
            });

            let output = '';
            let lastProgress = '';

            downloadProcess.stdout.on('data', (data) => {
                const text = data.toString();
                output += text;
                
                // Extract progress information
                const progressMatch = text.match(/pulling.*?(\d+%)/);
                if (progressMatch && progressMatch[1] !== lastProgress) {
                    lastProgress = progressMatch[1];
                    process.stdout.write(`\rüìä Progress: ${lastProgress}`);
                }
            });

            downloadProcess.stderr.on('data', (data) => {
                const text = data.toString();
                if (!text.includes('[?')) { // Filter out terminal control sequences
                    console.log(text);
                }
            });

            downloadProcess.on('close', (code) => {
                process.stdout.write('\n');
                if (code === 0) {
                    console.log(`‚úÖ ${modelName} downloaded successfully`);
                    resolve(true);
                } else {
                    console.log(`‚ùå Failed to download ${modelName}`);
                    reject(new Error(`Download failed with code ${code}`));
                }
            });

            downloadProcess.on('error', (error) => {
                console.log(`‚ùå Error downloading ${modelName}:`, error.message);
                reject(error);
            });
        });
    }

    async testModel(modelName) {
        console.log(`üß™ Testing ${modelName}...`);
        
        try {
            const response = await fetch(`${this.ollamaHost}/api/generate`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    model: modelName,
                    prompt: 'Say "Hello from IRIS" in exactly 3 words.',
                    stream: false
                })
            });

            const data = await response.json();
            if (data.response && data.response.includes('Hello')) {
                console.log(`‚úÖ ${modelName} is working correctly`);
                console.log(`   Response: ${data.response.trim()}`);
                return true;
            } else {
                console.log(`‚ö†Ô∏è  ${modelName} gave unexpected response: ${data.response}`);
                return false;
            }
        } catch (error) {
            console.log(`‚ùå Failed to test ${modelName}:`, error.message);
            return false;
        }
    }

    async setupDefaultModel() {
        console.log('\nüöÄ IRIS Local AI Setup Starting...\n');

        // Check prerequisites
        if (!await this.checkOllamaInstalled()) {
            return false;
        }

        if (!await this.checkOllamaRunning()) {
            return false;
        }

        // Check existing models
        const installedModels = await this.getInstalledModels();
        console.log(`üìã Currently installed models: ${installedModels.join(', ') || 'None'}`);

        // Download default model if not present
        if (!installedModels.includes(this.defaultModel)) {
            try {
                await this.downloadModel(this.defaultModel);
            } catch (error) {
                console.log(`‚ö†Ô∏è  Failed to download ${this.defaultModel}, trying backup models...`);
                
                // Try backup models
                for (const backupModel of this.backupModels) {
                    if (!installedModels.includes(backupModel)) {
                        try {
                            await this.downloadModel(backupModel);
                            console.log(`‚úÖ Using ${backupModel} as fallback model`);
                            break;
                        } catch (backupError) {
                            console.log(`‚ö†Ô∏è  ${backupModel} also failed, trying next...`);
                        }
                    }
                }
            }
        } else {
            console.log(`‚úÖ ${this.defaultModel} already installed`);
        }

        // Test the model
        const finalModels = await this.getInstalledModels();
        const modelToTest = finalModels.find(m => 
            m.includes('mistral') || m.includes('llama3.1') || m.includes('llama3.2')
        );

        if (modelToTest) {
            await this.testModel(modelToTest);
            console.log('\nüéâ Local AI setup completed successfully!');
            console.log(`ü§ñ Primary model: ${modelToTest}`);
            console.log('üöÄ IRIS is ready to use local AI');
            return true;
        } else {
            console.log('\n‚ùå No suitable AI model could be installed');
            return false;
        }
    }

    async run() {
        const success = await this.setupDefaultModel();
        process.exit(success ? 0 : 1);
    }
}

// Auto-run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
    const setup = new LocalAISetup();
    setup.run().catch(console.error);
}

export default LocalAISetup;